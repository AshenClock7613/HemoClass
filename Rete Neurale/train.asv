function [J, newWeights] = train(weights, layer_sizes, X, y_new, lambda, alfa)
    % Setup some useful variables
    m = size(X, 1);
    a1 = [ones(m, 1) X];   

    % You need to return the following variables correctly 
    J = 0;

    % initialize the sum and the activations
    for i = 1:(size(layer_sizes,2))
        z{i} = zeros(m,layer_sizes(i)+1);
        a{i} = zeros(m,layer_sizes(i)+1);
    end

    a{1,1} = a1;  

    %% FORWARD PROPAGATION
    for i = 2:size(layer_sizes,2)
        z{1,i} = a{1,i-1}*weights{1,i-1}';
        a{1,i} = sigmoid(z{1,i});
        if i ~= size(layer_sizes,2)
            a{1,i} = [ones(length(z{1,i}),1) a{1,i}];
        end
    end

    % cost function for classification
    J = sum(sum(-y_new .* log(a{1,end}) - (1 - y_new) .* log(1 - a{1,end}))) / m ;

    % add regularization to the cost function
    regularization = 0;
    for i = 1:(size(layer_sizes,2)-1)
        regularization = regularization + sum(weights{1,i}(:,2:end).^2,'all');
    end

    regularization =  lambda/(2*m) * regularization;
    J = J + regularization;

    %% BACK PROPAGATION 
    % sigmas
    for i = size(layer_sizes,2):-1:2
        if i == size(layer_sizes,2)
            s{i} = a{1,i} - y_new;
        else
            s{i} = (s{1,i+1}*weights{1,i}(:, 2:end)) .* sigmoidGradient(z{1,i});
        end
    end

    % deltas
    for i = 1:(size(layer_sizes,2)-1)
        deltas{i} = s{1,i+1}'*a{1,i};
    end

    %% GRADIENT DESCENT (CONJUGATE GRADIENT DESCENT)
    currentWeights = weights; % Initialize current weights

    % Initialize gradient and direction
    gradient = cell(size(weights));
    direction = cell(size(weights));
    for i = 1:(size(layer_sizes,2)-1)
        p = lambda * [zeros(size(currentWeights{1,i}, 1), 1), currentWeights{1,i}(:, 2:end)] / m;
        gradient{i} = deltas{1,i}./m + p;
        direction{i} = -gradient{i}; % Initialize direction as opposite of gradient
    end
    maxIterations = 100;
    % Conjugate gradient descent iterations
    for iter = 1:maxIterations % You need to define maxIterations
        % Update weights using current direction
        for i = 1:(size(layer_sizes,2)-1) 
            currentWeights{1,i}(:,1) = currentWeights{1,i}(:,1) - alfa*mean(direction{1,i}(:,1),1);
            currentWeights{1,i}(:,2:end) = currentWeights{1,i}(:,2:end) - alfa*direction{1,i}(:,2:end);
        end

        % Compute new gradient
        for i = 1:(size(layer_sizes,2)-1) 
            p = lambda * [zeros(size(currentWeights{1,i}, 1), 1), currentWeights{1,i}(:, 2:end)] / m;
            newGradient{i} = deltas{1,i}./m + p;
        end

        % Compute beta for conjugate direction
        beta = 0;
        for i = 1:(size(layer_sizes,2)-1)
            beta = beta + sum(newGradient{i}(:) .* (newGradient{i}(:) - gradient{i}(:))) / sum(gradient{i}(:).^2);
        end

        % Update conjugate direction
        for i = 1:(size(layer_sizes,2)-1) 
            direction{i} = -newGradient{i} + beta * direction{i};
        end

        % Convergence conditions
        % You should implement convergence conditions here
    end

    % Set new weights after conjugate gradient descent
    newWeights = currentWeights;
end
